{"data":{"featured":{"edges":[{"node":{"frontmatter":{"title":"BIGE","cover":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAHCAYAAAAIy204AAAACXBIWXMAAAsTAAALEwEAmpwYAAABr0lEQVQoz12Rz2sTURSF5x9zpS4UBF25918QXLlvF+JOF3VhQalp0aaClKQY0tIS0TY2aWZsMLVtbJw0P2by5s2bycy8mfkkoQp64HLhXPjgnmPkecZs/lee52RZNt8z1bf7rD6vUy60mQYJo7EkjiFLTkn9N6T+a9LIxMivAImeMlY9Rn6XOAn/gYdhQHX9jNVnhxSXTAa2wHE8ohgSf5NseINscG0ONRAtUEM6zlf2fxQ5ahTpmJsE04hms4kQHsKbcLhr8/LJHqWCRaBChHDnwDSsgrgP4i6Zeoex8eAh7aVlLKdG9+QLw0YD89MKE1+yuLBAuVxGSn/+ujMe4kmJUgGe56KCnER9xD6+jW1dJ/bWME47A/xJhDXYpt5+z/HBB1qfV0iB7vk5tv2LJNH0R0fU9kv0ehXiyCOMImbJ66CCuriH7N5Cy7cYf3Lqe9/ZaryidLBMz7Xmnu9LHMdFSpe9xg6FjR265jpO32LYbqH8lERtgXsH3JukqoCRZ/nfJtNUo1N9VTNorYnjmNn552WJyu5Tvp29wJlc4A4uESIkEDW0+xjtPEIHVX4DlYL8Jdixg/8AAAAASUVORK5CYII="},"images":{"fallback":{"src":"/static/64e2e0177d0f4c77dd9421594e4e0259/4ee0d/cover_image.png","srcSet":"/static/64e2e0177d0f4c77dd9421594e4e0259/cb0db/cover_image.png 175w,\n/static/64e2e0177d0f4c77dd9421594e4e0259/34177/cover_image.png 350w,\n/static/64e2e0177d0f4c77dd9421594e4e0259/4ee0d/cover_image.png 700w,\n/static/64e2e0177d0f4c77dd9421594e4e0259/ae11a/cover_image.png 1400w","sizes":"(min-width: 700px) 700px, 100vw"},"sources":[{"srcSet":"/static/64e2e0177d0f4c77dd9421594e4e0259/fbee0/cover_image.avif 175w,\n/static/64e2e0177d0f4c77dd9421594e4e0259/bd4a7/cover_image.avif 350w,\n/static/64e2e0177d0f4c77dd9421594e4e0259/b63fe/cover_image.avif 700w,\n/static/64e2e0177d0f4c77dd9421594e4e0259/f2ec1/cover_image.avif 1400w","type":"image/avif","sizes":"(min-width: 700px) 700px, 100vw"},{"srcSet":"/static/64e2e0177d0f4c77dd9421594e4e0259/55441/cover_image.webp 175w,\n/static/64e2e0177d0f4c77dd9421594e4e0259/8a272/cover_image.webp 350w,\n/static/64e2e0177d0f4c77dd9421594e4e0259/f2ae7/cover_image.webp 700w,\n/static/64e2e0177d0f4c77dd9421594e4e0259/9137d/cover_image.webp 1400w","type":"image/webp","sizes":"(min-width: 700px) 700px, 100vw"}]},"width":700,"height":253}}},"tech":["GenAI","Biomechanics","Score Guidance","Sports analytics","Injury prevention"],"github":"https://github.com/Rose-STL-Lab/BIGE","external":null,"cta":"https://rose-stl-lab.github.io/UCSD-OpenCap-Fitness-Dataset/"},"html":"<p>BIGE is a framework for generative models to adhere to clinician-defined constraints.\nTo generate realistic motion, our method uses a biomechanically informed surrogate model to guide the generation process.</p>"}},{"node":{"frontmatter":{"title":"MoRAG","cover":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAAFCAYAAABFA8wzAAAACXBIWXMAAknwAAJJ8AGtv6cTAAABHklEQVQY043POU/CcACG8X53VycHBxMNCiqCcTEEOWIiERskWARFKIcFLLS0HMrR4194TCSuxt/ybG/yShu29OmUpFLn4bVHQ5/Q7nQYjmwS9woVVeO/pG02GAOLaCjDwd4VuWyZTCZD462BnMpSLZZoqSNKpQq3uRxqq/33oOu62LZB7GKf49Aucv6GdDpFv6vxopR4b6p89E0qlSpyoUCzXmWiqzgzm+V4hPs1w1/OEYtPJNMcomldBqZOrf9MufNIz+owNAws2yJfLFBT6z8vfom5iRzfoXx3TTJySCp+SiIaJhsPI1WfZEQQIHxB4K/ZBCD8gHWwxvM8AiHwPR/HcVitVjiux2I2YqwppJIJTo4OiZ1FOI+EuYzF+AaZ42p1OgPaIwAAAABJRU5ErkJggg=="},"images":{"fallback":{"src":"/static/69c74e87481eabfd4ab7ba2566bb1fee/17832/cover_image.png","srcSet":"/static/69c74e87481eabfd4ab7ba2566bb1fee/cd5f8/cover_image.png 175w,\n/static/69c74e87481eabfd4ab7ba2566bb1fee/a95d4/cover_image.png 350w,\n/static/69c74e87481eabfd4ab7ba2566bb1fee/17832/cover_image.png 700w","sizes":"(min-width: 700px) 700px, 100vw"},"sources":[{"srcSet":"/static/69c74e87481eabfd4ab7ba2566bb1fee/5cdb9/cover_image.avif 175w,\n/static/69c74e87481eabfd4ab7ba2566bb1fee/54f4c/cover_image.avif 350w,\n/static/69c74e87481eabfd4ab7ba2566bb1fee/8f771/cover_image.avif 700w","type":"image/avif","sizes":"(min-width: 700px) 700px, 100vw"},{"srcSet":"/static/69c74e87481eabfd4ab7ba2566bb1fee/ad15a/cover_image.webp 175w,\n/static/69c74e87481eabfd4ab7ba2566bb1fee/39e3c/cover_image.webp 350w,\n/static/69c74e87481eabfd4ab7ba2566bb1fee/c99b3/cover_image.webp 700w","type":"image/webp","sizes":"(min-width: 700px) 700px, 100vw"}]},"width":700,"height":183}}},"tech":["WACV 25'","Motion Diffusion","RAG","Zero-Shot"],"github":"https://github.com/Motion-RAG/MoRAG","external":null,"cta":"https://motion-rag.github.io/"},"html":"<p>A multi-part fusion based retrieval-augmented generation strategy for text-based human motion generation.\nBy effectively prompting large language models (LLMs), we address spelling errors and rephrasing issues in motion retrieval.\nOur approach utilizes a multi-part retrieval strategy to improve the generalizability of motion retrieval across the language space.</p>"}},{"node":{"frontmatter":{"title":"Frugal Motion Capture","cover":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAIABQDASIAAhEBAxEB/8QAFwABAAMAAAAAAAAAAAAAAAAAAAECBf/EABUBAQEAAAAAAAAAAAAAAAAAAAAB/9oADAMBAAIQAxAAAAHZkSwP/8QAFhABAQEAAAAAAAAAAAAAAAAAQQAC/9oACAEBAAEFAkzf/8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAwEBPwE//8QAFBEBAAAAAAAAAAAAAAAAAAAAEP/aAAgBAgEBPwE//8QAGRAAAQUAAAAAAAAAAAAAAAAAMQABAhAy/9oACAEBAAY/An0ok1//xAAZEAACAwEAAAAAAAAAAAAAAAAAATFBcWH/2gAIAQEAAT8haeKmcNC0zZ//2gAMAwEAAgADAAAAEAvf/8QAFREBAQAAAAAAAAAAAAAAAAAAAAH/2gAIAQMBAT8QqP/EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8QP//EABwQAQACAgMBAAAAAAAAAAAAAAERIQAxUWHw8f/aAAgBAQABPxCfJB+HjEiIaUpTe+ssXcMt6s//2Q=="},"images":{"fallback":{"src":"/static/b8fafe17ef2c7c4ca5b592f4d75e29b1/49462/cover_image.jpg","srcSet":"/static/b8fafe17ef2c7c4ca5b592f4d75e29b1/8df76/cover_image.jpg 175w,\n/static/b8fafe17ef2c7c4ca5b592f4d75e29b1/2761d/cover_image.jpg 350w,\n/static/b8fafe17ef2c7c4ca5b592f4d75e29b1/49462/cover_image.jpg 700w,\n/static/b8fafe17ef2c7c4ca5b592f4d75e29b1/312ea/cover_image.jpg 1400w","sizes":"(min-width: 700px) 700px, 100vw"},"sources":[{"srcSet":"/static/b8fafe17ef2c7c4ca5b592f4d75e29b1/cdf5b/cover_image.avif 175w,\n/static/b8fafe17ef2c7c4ca5b592f4d75e29b1/a5acc/cover_image.avif 350w,\n/static/b8fafe17ef2c7c4ca5b592f4d75e29b1/c4b2f/cover_image.avif 700w,\n/static/b8fafe17ef2c7c4ca5b592f4d75e29b1/70947/cover_image.avif 1400w","type":"image/avif","sizes":"(min-width: 700px) 700px, 100vw"},{"srcSet":"/static/b8fafe17ef2c7c4ca5b592f4d75e29b1/9566d/cover_image.webp 175w,\n/static/b8fafe17ef2c7c4ca5b592f4d75e29b1/8c253/cover_image.webp 350w,\n/static/b8fafe17ef2c7c4ca5b592f4d75e29b1/58e6f/cover_image.webp 700w,\n/static/b8fafe17ef2c7c4ca5b592f4d75e29b1/181e6/cover_image.webp 1400w","type":"image/webp","sizes":"(min-width: 700px) 700px, 100vw"}]},"width":700,"height":284}}},"tech":["CVPR 23'","Motion Capture","Skeletons","Deformation Transfer"],"github":"https://transfer4d.github.io/","external":null,"cta":"https://transfer4d.github.io/"},"html":"<p>Transfer4D transfers motion from a commodity depth sensor to a virtual model. The goal of our work is to democratize animation transfer using commodity depth sensors and alleviate the animators effort by automating the rigging and animation transfer process.</p>"}},{"node":{"frontmatter":{"title":"Action-GPT","cover":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAARCAYAAADdRIy+AAAACXBIWXMAAAAAAAAD6AF+hNEZAAADUElEQVQ4y3WT/VMbZRDH7z/1H3D8wVF0Wi2CUysZHYRitWWAdgRrgpXWUsmgA6VWSgkvIQGSQDhyyV3Ckct77iWXSz7O3cExTHVnnnlm99n97u53nxX6/T4TMws8jb7GlV6vj2tzz3/Jpf3yNm2H6FaKiYWXSGcaQiYrsrmT5O3OPqJUwLa7uL5uQLVaRVXPaTWbFAoFNE0LwHoBYJc7kRXeC82xe5xHqNZ1lLMajmNjWh3XPajGBSsVC8hKiWKxSLvduladK3a3z0FeZf1QQmu0EFY3VH4My+hm76Jl2D2s02h1PF0qWswtlshKegDmHtt2eLGm8ls0e40SwXX87LsUJ7myZ3j06yHv33zmte5KLFHhi7uHJNOqR8Vlq7YDAyNbjE1tenrX6XmJBLFgcfteEsMwvYfBsT0WokdBxpNCh8HRGD3HDmyJRIKlpSWGxrf5fupPdrY2vGF6FZ4WWkz9EqfZ8gGfRPNsJ9UgOCs1GJ1cxzC7Ab/hcJiBgU/56NYsnw9PMPngBxzHp0wonhvEkhXahuOVHEuUOcm3rgajmaxvK1idXgAYiYS5cXOQG0P3GRwKMTM9jeM4PmC+UCR5cIxWqXkcZU+L5CQlcJCLZbKijNWx4YK/eDzO8+eLRJdXiEaX2Xjz5moo56pKOpNFbzf9FkWFnJT3J+44VLQyqXQWQ28GQW4yRSkiywVEUaTRaAQ/QMifdYlEZc+glKo8XpR4vXvVclpsMvt7nmrduPZt/m+DBDf445E4atnN0uPZisb4T+IFX33+2akwOJ5EksvXAPbSNeqt3jugwl5a5/68Qr/vPy4sK0zPpwKnlGgwNLZJv+9z2u12MfQ2L9dP+CS0xfIrCVOvY1n+IgiZkyo/P92n3vS/zV9/n/Lkj8wVYLbKg7kYuuEHpDNHhEJ3GL37kA+/fMkHt14w/NU4a2uvfMBqQ2fn4Ixay/IMySON9d1SACirOvF0BbPjV5hOZwiFRgh9e49vRif5evQxw7fHWV1d9QE1ucTxfgq72XL3h/3tBKn4hQ7U1RpK7ox6vXHVsmGgaWVisbcocg7L1LEsvyAhbJwzU8kxb2nMNhWmNZHpssijWp6Iec7Dep5IOYdhmu9Mtt3Wg52/HMq/D5bagPb1M1AAAAAASUVORK5CYII="},"images":{"fallback":{"src":"/static/a791edd0dac3629f450bac130f6efd54/40d0f/cover_image.png","srcSet":"/static/a791edd0dac3629f450bac130f6efd54/25f13/cover_image.png 175w,\n/static/a791edd0dac3629f450bac130f6efd54/1f545/cover_image.png 350w,\n/static/a791edd0dac3629f450bac130f6efd54/40d0f/cover_image.png 700w,\n/static/a791edd0dac3629f450bac130f6efd54/98979/cover_image.png 1400w","sizes":"(min-width: 700px) 700px, 100vw"},"sources":[{"srcSet":"/static/a791edd0dac3629f450bac130f6efd54/fdd97/cover_image.avif 175w,\n/static/a791edd0dac3629f450bac130f6efd54/60a92/cover_image.avif 350w,\n/static/a791edd0dac3629f450bac130f6efd54/eb3fa/cover_image.avif 700w,\n/static/a791edd0dac3629f450bac130f6efd54/fba70/cover_image.avif 1400w","type":"image/avif","sizes":"(min-width: 700px) 700px, 100vw"},{"srcSet":"/static/a791edd0dac3629f450bac130f6efd54/09ff3/cover_image.webp 175w,\n/static/a791edd0dac3629f450bac130f6efd54/ab833/cover_image.webp 350w,\n/static/a791edd0dac3629f450bac130f6efd54/83865/cover_image.webp 700w,\n/static/a791edd0dac3629f450bac130f6efd54/ab1e8/cover_image.webp 1400w","type":"image/webp","sizes":"(min-width: 700px) 700px, 100vw"}]},"width":700,"height":581}}},"tech":["ICME 23'","Motion Synthesis","GPT","Zero-Shot"],"github":"https://github.com/actiongpt/actiongpt","external":null,"cta":"https://actiongpt.github.io/"},"html":"<p>A plug and play framework for incorporating Large Language Models (LLMs) into text-based action generation models. By carefully crafting prompts for LLMs, we generate richer and fine-grained descriptions of the action. We show that utilizing these detailed descriptions instead of the original action phrases leads to better alignment of text and motion spaces.</p>"}},{"node":{"frontmatter":{"title":"Skeleton Action Generation","cover":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAALABQDASIAAhEBAxEB/8QAFwABAQEBAAAAAAAAAAAAAAAAAAIDBf/EABUBAQEAAAAAAAAAAAAAAAAAAAAB/9oADAMBAAIQAxAAAAHsVF2Go//EABgQAAIDAAAAAAAAAAAAAAAAAAAQAREy/9oACAEBAAEFAlRGl//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8BP//EABURAQEAAAAAAAAAAAAAAAAAAAAR/9oACAECAQE/AVf/xAAYEAADAQEAAAAAAAAAAAAAAAAAATEQMv/aAAgBAQAGPwKM5INb/8QAGxABAAEFAQAAAAAAAAAAAAAAAQAQETFB8CH/2gAIAQEAAT8hDWjEtxh2z0jAEsU//9oADAMBAAIAAwAAABCf3//EABYRAQEBAAAAAAAAAAAAAAAAAAABEf/aAAgBAwEBPxBkf//EABYRAQEBAAAAAAAAAAAAAAAAAAEQIf/aAAgBAgEBPxAwj//EAB0QAAICAQUAAAAAAAAAAAAAAAERACExEEFxgfD/2gAIAQEAAT8QyMIAxx3vC5NrcAtkII3o8By47I89P//Z"},"images":{"fallback":{"src":"/static/a339dd47cbf8addd3c9f423f2618d854/7e52b/cover_image.jpg","srcSet":"/static/a339dd47cbf8addd3c9f423f2618d854/7e1f9/cover_image.jpg 175w,\n/static/a339dd47cbf8addd3c9f423f2618d854/3f538/cover_image.jpg 350w,\n/static/a339dd47cbf8addd3c9f423f2618d854/7e52b/cover_image.jpg 700w,\n/static/a339dd47cbf8addd3c9f423f2618d854/df16c/cover_image.jpg 1400w","sizes":"(min-width: 700px) 700px, 100vw"},"sources":[{"srcSet":"/static/a339dd47cbf8addd3c9f423f2618d854/2f9bd/cover_image.avif 175w,\n/static/a339dd47cbf8addd3c9f423f2618d854/8024a/cover_image.avif 350w,\n/static/a339dd47cbf8addd3c9f423f2618d854/564a2/cover_image.avif 700w,\n/static/a339dd47cbf8addd3c9f423f2618d854/ea4a7/cover_image.avif 1400w","type":"image/avif","sizes":"(min-width: 700px) 700px, 100vw"},{"srcSet":"/static/a339dd47cbf8addd3c9f423f2618d854/9b040/cover_image.webp 175w,\n/static/a339dd47cbf8addd3c9f423f2618d854/3d0d8/cover_image.webp 350w,\n/static/a339dd47cbf8addd3c9f423f2618d854/0ffb2/cover_image.webp 700w,\n/static/a339dd47cbf8addd3c9f423f2618d854/accb3/cover_image.webp 1400w","type":"image/webp","sizes":"(min-width: 700px) 700px, 100vw"}]},"width":700,"height":370}}},"tech":["WACV 21'","VAE","Class-conditioning","Kinematics"],"github":"https://github.com/skelemoa/dsag","external":null,"cta":"https://skeleton.iiit.ac.in/dsag"},"html":"<p>MUGL is a deep neural model for large-scale, variable-length, diverse generation of single and multi-person pose-based action sequences with locomotion. To enable intra/inter-category diversity, we model the latent generative space as a Conditional Gaussian Mixture Variational Autoencoder. We use a hybrid pose sequence representation with 3D pose sequences sourced from videos and 3D Kinect-based sequences of NTU-RGBD-120. Although smaller and simpler compared to baselines, MUGL outperforms the baselines across multiple generative model metrics</p>"}},{"node":{"frontmatter":{"title":"Full Body Action Generation","cover":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAKABQDASIAAhEBAxEB/8QAGQAAAgMBAAAAAAAAAAAAAAAAAAECAwQF/8QAFAEBAAAAAAAAAAAAAAAAAAAAAP/aAAwDAQACEAMQAAAB6SvRA0B//8QAGRAAAwEBAQAAAAAAAAAAAAAAAAIRASEx/9oACAEBAAEFAicVaTCYJ5//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAEDAQE/AT//xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/AT//xAAZEAACAwEAAAAAAAAAAAAAAAAAMgEQIjH/2gAIAQEABj8CnY4xy//EABoQAQADAQEBAAAAAAAAAAAAAAEAESExQaH/2gAIAQEAAT8hRcBsoomPXYoU+JwxGjh2Csz/2gAMAwEAAgADAAAAEHQf/8QAFREBAQAAAAAAAAAAAAAAAAAAABH/2gAIAQMBAT8Qqv/EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8QP//EAB8QAAEEAQUBAAAAAAAAAAAAAAEAESExQVFhcYGx8P/aAAgBAQABPxABJPBuzH7VA2SdbtUUjUSH8goCgCsLvLWwQoAE44C//9k="},"images":{"fallback":{"src":"/static/8dc74cd6be94a9f2d0699886f6bacc94/57c8c/dsag-teaser.jpg","srcSet":"/static/8dc74cd6be94a9f2d0699886f6bacc94/5afec/dsag-teaser.jpg 175w,\n/static/8dc74cd6be94a9f2d0699886f6bacc94/d518d/dsag-teaser.jpg 350w,\n/static/8dc74cd6be94a9f2d0699886f6bacc94/57c8c/dsag-teaser.jpg 700w,\n/static/8dc74cd6be94a9f2d0699886f6bacc94/48eb2/dsag-teaser.jpg 1400w","sizes":"(min-width: 700px) 700px, 100vw"},"sources":[{"srcSet":"/static/8dc74cd6be94a9f2d0699886f6bacc94/b2baf/dsag-teaser.avif 175w,\n/static/8dc74cd6be94a9f2d0699886f6bacc94/3ba2a/dsag-teaser.avif 350w,\n/static/8dc74cd6be94a9f2d0699886f6bacc94/c67ab/dsag-teaser.avif 700w,\n/static/8dc74cd6be94a9f2d0699886f6bacc94/7eebf/dsag-teaser.avif 1400w","type":"image/avif","sizes":"(min-width: 700px) 700px, 100vw"},{"srcSet":"/static/8dc74cd6be94a9f2d0699886f6bacc94/d8229/dsag-teaser.webp 175w,\n/static/8dc74cd6be94a9f2d0699886f6bacc94/3668f/dsag-teaser.webp 350w,\n/static/8dc74cd6be94a9f2d0699886f6bacc94/dfc53/dsag-teaser.webp 700w,\n/static/8dc74cd6be94a9f2d0699886f6bacc94/ccf6e/dsag-teaser.webp 1400w","type":"image/webp","sizes":"(min-width: 700px) 700px, 100vw"}]},"width":700,"height":361}}},"tech":["WACV 22'","SMPL","ExPose","Self-Attention","VAE"],"github":"https://github.com/skelemoa/dsag","external":null,"cta":"https://skeleton.iiit.ac.in/dsag"},"html":"<p>DSAG is a controllable deep neural framework for action-conditioned generation of full body actions. To overcome shortcomings in existing generative approaches, we introduce dedicated representations for encoding finger joints. We also introduce novel spatiotemporal transformation blocks with multi-head self attention and specialized temporal processing. The design choices enable generations for a large range in body joint counts (24 - 52), frame rates (13 - 50), global body movement (inplace, locomotion) and action categories (12 - 120), across multiple datasets (NTU-120, HumanAct12, UESTC, Human3.6M).</p>"}},{"node":{"frontmatter":{"title":"Recognition using multi-modal perception system","cover":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/webp;base64,UklGRswAAABXRUJQVlA4IMAAAADwBACdASoUABEAPtFgqU+oJSOiKAgBABoJZQCuHYv0d4bFdF7Z4VZqD2jxdy0hE4AA/s8sco5wC5dVnwM9XXNymIYNJkl9+eP6B4AcPBPeMBuDvJ3Q4lXMfLczPLCwc08n3xFjnFsymQsMjYPdQUeA1RyPfotTaapQKk4q4Wch9nJrGm7rnCuUaUI9///ifg7Sypa4V1+6WUycc5iu5GI2I+t+UnH72MbCq4kz/g0xDgQULgTeQvgBVD5hsGwAAAA="},"images":{"fallback":{"src":"/static/26a7859696a9bf0bfefd76a3a66087c1/a777d/final.webp","srcSet":"/static/26a7859696a9bf0bfefd76a3a66087c1/ac71d/final.webp 150w,\n/static/26a7859696a9bf0bfefd76a3a66087c1/c233c/final.webp 300w,\n/static/26a7859696a9bf0bfefd76a3a66087c1/a777d/final.webp 600w","sizes":"(min-width: 600px) 600px, 100vw"},"sources":[{"srcSet":"/static/26a7859696a9bf0bfefd76a3a66087c1/3d662/final.avif 150w,\n/static/26a7859696a9bf0bfefd76a3a66087c1/09c05/final.avif 300w,\n/static/26a7859696a9bf0bfefd76a3a66087c1/d7970/final.avif 600w","type":"image/avif","sizes":"(min-width: 600px) 600px, 100vw"}]},"width":700,"height":591.5}}},"tech":["Gait Recognition","Face Recognition","Human Re-identification"],"github":"https://github.com/robocomp/robocomp-aston/tree/master/components/detection/HumanIdentification","external":null,"cta":"https://www.youtube.com/watch/Lt7oiOuUIJA"},"html":"<p>Identify humans using multiple methods modalities, such as rgb image, facial features and silhouette of the body.\nSystem design for communication between different.</p>"}},{"node":{"frontmatter":{"title":"Document Tampering Detection","cover":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/png;base64,iVBORw0KGgoAAAANSUhEUgAAABQAAAARCAIAAABSJhvpAAAACXBIWXMAAAsTAAALEwEAmpwYAAAEDklEQVQ4yy3O6U+bdQDA8Z8mvpC5F8Y4fGFcTExMDJBsyzjEuUDcxdFNGsKGmRI2AwuwAyZzjLEgKhNIYAMzr0TtpgxEDrvSUkqfnvR6ej6lT5/26dP7oC29b9o9xuAf8Mn3C+QJUpEihcE8359TJElxJC8K7S4jtqdSw6wMnYMxUXCX60tD3jQ/kOP5s/ztLOTPQoEc15cBy0uLjx/TdKiecDqf0H5XKORMiH/t3q3h8ZFH30/OTE9MTE/F0ilv2Om0oC47RnjMbhfuI7CdqBucLD/U3lKnlT4jtiSDX/R8M3Br8ocHr79zsKrug8bGCkrdiaPlpbFIMIbKdAIGLOOIhQxYwhJzlo16PjhPqbv7OZV+v2txrO/bwb7J70ZXuX/XnPjwApXa0dba0kjpvXQhFg+hWpHFIMLlTxHhn2YN3emQWHEZoJyuHWg9tXz38sJgZ3dLw9TY6K8LtLcOv93Q2nC7v/si9dzwwNWAy/po+NMfx7oUMFNv2sDt4kgKy+ad4Oyp4w3VhybbKPfbqTVHSye+Hvp5jvZu05Ez/U0PpiZu97Zd7z6fDDht8zf1jJvRhD2TdecLXi73n/m538AnF7uOH6sdG7o62NvZc71vfHx6njHb2tN6bbDjy/7ue1/d6OxujkYCUVwRtMJb8AYiY2NqiLX4xwLtJ4ChFpVMajFuGREkEo0Fwym3C1cqREqZRACxEbUSNWoKu5nCTjCfjPp9DrsFCwe86UQonQgBU5YkCqQuUTCknhuSpD5eMKZJbTC+jhCQ2a6PJ2wkqQ7ntHFSnyCR2HNNOG/MkOZd0pghgSkQ05nMagOqIZy2SMoWTamsLilqVuHEKhtdous5kNW0HTX5QzITIcftStzOgbXMTVhqJIDfYVWohTwhE7NiuVw2nU1BCh5LvOaPBMbbDSdfYzVXyHdsScxleMaj85R8Dpf9ZnHxSy++cKz6fSBjL0uFm9vBeHAnHIpE45kkVyNaEbE8Ud8ARVtTIW+oZPnwhMGNMjbZEgzeEPCKXi4CAJSUlADeyhIshFAM4TLXEJEinorDFu2mEd6O+e7UayuLVj9+Y91nS+qsOpZ0XYGrN/jQ/lf2AwDKysqAdiu6Sic4MyrmX2pe0BNLxOWYSm5WeSL+h1R6OXh87iAUDKX1BLIm31BatAKJaF/Rvv/xlYdT1adHq648AY6V90iJNxkR6yQCROIJe0co5irAbi6G/O406kTFeomK0PLFgr3tyspKsFJTfqP+TO087cDMykfzXH88KkUVPI3YE/bcqTdWAEb9q2t+R9q2Y93ckqE+XCyX7uH/yrO/jHxGHz47hFw+LOqgafzxMIwpuSqhJ+SZuKRvOgB3HtGFvDmDw7AqZsswtUAi3tsuLS39F6NFr6R7iYhgAAAAAElFTkSuQmCC"},"images":{"fallback":{"src":"/static/8758404f0bd75c31dd7300e564db8620/01f6f/document_tampering.png","srcSet":"/static/8758404f0bd75c31dd7300e564db8620/c9c96/document_tampering.png 175w,\n/static/8758404f0bd75c31dd7300e564db8620/067e6/document_tampering.png 350w,\n/static/8758404f0bd75c31dd7300e564db8620/01f6f/document_tampering.png 700w","sizes":"(min-width: 700px) 700px, 100vw"},"sources":[{"srcSet":"/static/8758404f0bd75c31dd7300e564db8620/50f46/document_tampering.avif 175w,\n/static/8758404f0bd75c31dd7300e564db8620/5c157/document_tampering.avif 350w,\n/static/8758404f0bd75c31dd7300e564db8620/203d9/document_tampering.avif 700w","type":"image/avif","sizes":"(min-width: 700px) 700px, 100vw"},{"srcSet":"/static/8758404f0bd75c31dd7300e564db8620/338eb/document_tampering.webp 175w,\n/static/8758404f0bd75c31dd7300e564db8620/074db/document_tampering.webp 350w,\n/static/8758404f0bd75c31dd7300e564db8620/ddd49/document_tampering.webp 700w","type":"image/webp","sizes":"(min-width: 700px) 700px, 100vw"}]},"width":700,"height":587}}},"tech":["Patch Match","Camera Noise","MATLAB"],"github":"https://github.com/shubhMaheshwari/documentImageTampering","external":null,"cta":"https://shubhmaheshwari.github.io/documentImageTampering/"},"html":"<p>Finding fake identity cards, receipts, and text using noise pattern of camera and patch matching to detection of the photoshopped region in document</p>"}},{"node":{"frontmatter":{"title":"College Tour Guide Robot","cover":{"childImageSharp":{"gatsbyImageData":{"layout":"constrained","placeholder":{"fallback":"data:image/jpeg;base64,/9j/2wBDABALDA4MChAODQ4SERATGCgaGBYWGDEjJR0oOjM9PDkzODdASFxOQERXRTc4UG1RV19iZ2hnPk1xeXBkeFxlZ2P/2wBDARESEhgVGC8aGi9jQjhCY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2NjY2P/wgARCAAKABQDASIAAhEBAxEB/8QAFwAAAwEAAAAAAAAAAAAAAAAAAAMEAf/EABUBAQEAAAAAAAAAAAAAAAAAAAAB/9oADAMBAAIQAxAAAAFOIVFBUL//xAAcEAABBAMBAAAAAAAAAAAAAAABAAIDMQQREzL/2gAIAQEAAQUCdIZFi67vYSaQqHz/AP/EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQMBAT8BP//EABQRAQAAAAAAAAAAAAAAAAAAABD/2gAIAQIBAT8BP//EABsQAAICAwEAAAAAAAAAAAAAAAAhARESYYFx/9oACAEBAAY/AsSp3bEuCJ8On//EABsQAQACAwEBAAAAAAAAAAAAAAEAESExQVGh/9oACAEBAAE/IU6FI6Iu/ixsMs9hqi0OsH0n0pvj/9oADAMBAAIAAwAAABAbz//EABURAQEAAAAAAAAAAAAAAAAAAAAB/9oACAEDAQE/EEf/xAAUEQEAAAAAAAAAAAAAAAAAAAAQ/9oACAECAQE/ED//xAAcEAEBAQEAAgMAAAAAAAAAAAABEQAhMUGhsdH/2gAIAQEAAT8QIU0WrZA5muYl80uYEgCIXHJdkh9D4wCAH6G6r3r6N//Z"},"images":{"fallback":{"src":"/static/0482b81cc81fb9c4584dfa795c262c4c/c46dc/demo.jpg","srcSet":"/static/0482b81cc81fb9c4584dfa795c262c4c/3ea88/demo.jpg 175w,\n/static/0482b81cc81fb9c4584dfa795c262c4c/79b7e/demo.jpg 350w,\n/static/0482b81cc81fb9c4584dfa795c262c4c/c46dc/demo.jpg 700w,\n/static/0482b81cc81fb9c4584dfa795c262c4c/50ede/demo.jpg 1400w","sizes":"(min-width: 700px) 700px, 100vw"},"sources":[{"srcSet":"/static/0482b81cc81fb9c4584dfa795c262c4c/19d98/demo.avif 175w,\n/static/0482b81cc81fb9c4584dfa795c262c4c/e0303/demo.avif 350w,\n/static/0482b81cc81fb9c4584dfa795c262c4c/bb74c/demo.avif 700w,\n/static/0482b81cc81fb9c4584dfa795c262c4c/3113b/demo.avif 1400w","type":"image/avif","sizes":"(min-width: 700px) 700px, 100vw"},{"srcSet":"/static/0482b81cc81fb9c4584dfa795c262c4c/eac4f/demo.webp 175w,\n/static/0482b81cc81fb9c4584dfa795c262c4c/f1df2/demo.webp 350w,\n/static/0482b81cc81fb9c4584dfa795c262c4c/76c39/demo.webp 700w,\n/static/0482b81cc81fb9c4584dfa795c262c4c/3b966/demo.webp 1400w","type":"image/webp","sizes":"(min-width: 700px) 700px, 100vw"}]},"width":700,"height":354}}},"tech":["ROS","React Native","YOLO V3","Kinect","Speech2Text"],"github":"https://github.com/shubhMaheshwari/CVIT-Robot","external":null,"cta":"https://youtu.be/I3Pdhcx2EWo"},"html":"<p>Wheelchair Semi-humanoid robot which recognizes people and gives a tour of the college. The agent can track, tell jokes, listen and chat with the user. Integrated with a mobile application, ROS operating system, and YOLO V3</p>"}}]}}}